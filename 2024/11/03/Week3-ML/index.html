<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Zhen Xie">





<title>Week3 ML | Hexo</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>
<script>
    !
    function() {
    function n(n, e, t) {
    return n.getAttribute(e) || t
    }
    function e(n) {
    return document.getElementsByTagName(n)
    }
    function t() {
    var t = e("script"),
    o = t.length,
    i = t[o - 1];
    return {
    l: o,
    z: n(i, "zIndex", -1), //置于主页面背后
    o: n(i, "opacity", .5), //线条透明度
    c: n(i, "color", "0,0,0"), //线条颜色
    n: n(i, "count", 100) //线条数量
    }
    }
    function o() {
    a = m.width = window.innerWidth ||
    document.documentElement.clientWidth || document.body.clientWidth,
    c = m.height = window.innerHeight ||
    document.documentElement.clientHeight || document.body.clientHeight
    }
    function i() {
    r.clearRect(0, 0, a, c);
    var n, e, t, o, m, l;
    s.forEach(function(i, x) {
    for (i.x += i.xa, i.y += i.ya, i.xa *= i.x > a || i.x < 0 ? -1 :
    1, i.ya *= i.y > c || i.y < 0 ? -1 : 1, r.fillRect(i.x - .5, i.y - .5, 1,
    1), e = x + 1; e < u.length; e++) n = u[e],
    null !== n.x && null !== n.y && (o = i.x - n.x, m = i.y - n.y, l
    = o * o + m * m, l < n.max && (n === y && l >= n.max / 2 && (i.x -= .03 * o,
    i.y -= .03 * m), t = (n.max - l) / n.max, r.beginPath(), r.lineWidth = t /
    2, r.strokeStyle = "rgba(" + d.c + "," + (t + .2) + ")", r.moveTo(i.x, i.y),
    r.lineTo(n.x, n.y), r.stroke()))
    }),
    x(i)
    }
    var a, c, u, m = document.createElement("canvas"),
    d = t(),
    l = "c_n" + d.l,
    r = m.getContext("2d"),
    x = window.requestAnimationFrame || window.webkitRequestAnimationFrame
    || window.mozRequestAnimationFrame || window.oRequestAnimationFrame ||
    window.msRequestAnimationFrame ||
    function(n) {
    window.setTimeout(n, 1e3 / 45)
    },
    w = Math.random,
    y = {
    x: null,
    y: null,
    max: 2e4
    };
    m.id = l,
    m.style.cssText = "position:fixed;top:0;left:0;z-index:" + d.z +
    ";opacity:" + d.o,
    e("body")[0].appendChild(m),
    o(),
    window.onresize = o,
    window.onmousemove = function(n) {
    n = n || window.event,
    y.x = n.clientX,
    y.y = n.clientY
    },
    window.onmouseout = function() {
    y.x = null,
    y.y = null
    };
    for (var s = [], f = 0; d.n > f; f++) {
    var h = w() * a,
    g = w() * c,
    v = 2 * w() - 1,
    p = 2 * w() - 1;
    s.push({
    x: h,
    y: g,
    xa: v,
    ya: p,
    max: 6e3
    })
    }
    u = s.concat([y]),
    setTimeout(function() {
    i()
    },
    100)
    } ();
    </script>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">TheXie&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">TheXie&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">Week3 ML</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Zhen Xie</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">November 3, 2024&nbsp;&nbsp;15:04:20</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/basic/">basic</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h4 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h4><p><img src="/2024/11/03/Week3-ML/image-20241024183832021.png" alt="image-20241024183832021"></p>
<p>输入层–&gt;隐藏层–&gt;输出层</p>
<p>（神经网络&#x3D;自动化特征工程+逻辑&#x2F;线性回归）</p>
<p>神经网络的一大好处是从数据中训练它时无需明确决定要做什么，神经网络会自行计算出它想在这个隐藏层中使用的特征是什么</p>
<p>神经元（sigmoid）做的工作类似于逻辑回归，根据输入值输出一个0~1之间的数</p>
<p><img src="/2024/11/03/Week3-ML/image-20241024185906312.png" alt="image-20241024185906312"></p>
<h5 id="前向传播："><a href="#前向传播：" class="headerlink" title="前向传播："></a>前向传播：</h5><p>从左到右像如下图一样，将输入x–&gt;a^[1]^–&gt;a^[2]^–&gt;a^[3]^进行计算，叫做前向传播</p>
<p><img src="/2024/11/03/Week3-ML/image-20241024194324881.png" alt="image-20241024194324881"></p>
<p>步骤：</p>
<p>（1）定义每层</p>
<p>（2）连接每层</p>
<p>（3）model.compile</p>
<p>（4）model.fit(x,y)</p>
<p>（5）预测结果：model.predict(x_new)</p>
<p><img src="/2024/11/03/Week3-ML/image-20241024201144672.png" alt="image-20241024201144672"></p>
<h5 id="Tensorflow："><a href="#Tensorflow：" class="headerlink" title="Tensorflow："></a>Tensorflow：</h5><p>注意：矩阵在Numpy中的数据类型，如果放在Tensorflow中，数据类型会发生改变，转换为自己的内部格式tensor。如果想改回numpy中的数据类型，可以使用a1.numpy()来进行改类型：如图</p>
<p><img src="/2024/11/03/Week3-ML/image-20241024200602160.png" alt="image-20241024200602160"></p>
<p> np.dot</p>
<p>如果需要进行点积运算（内积），或者希望执行逐元素的乘法运算，可以使用<code>np.dot</code>,对于矩阵乘法，<code>np.dot</code>也可以使用</p>
<p>np.matmul</p>
<p>Z&#x3D;np.matmul(AT(A的转置),w)</p>
<p>等价于Z&#x3D;AT@w</p>
<p>如果需要进行矩阵乘法运算，确保参与运算的数组满足矩阵乘法的规则，可以使用<code>np.matmul</code>。<code>np.matmul</code>可以更好地处理矩阵乘法中的维度不匹配的情况，因此可以更清晰地表示矩阵乘法的规则。</p>
<h6 id="Tensorflow实现："><a href="#Tensorflow实现：" class="headerlink" title="Tensorflow实现："></a>Tensorflow实现：</h6><p>（1）定义连接每层（类比定义w1，w2….）</p>
<p>（2）选择编译方式（类比定义损失函数）</p>
<p>（3）选择epoch的值（类比梯度下降）</p>
<p>tips：BinaryCrossentropy（二元交叉熵）</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029164446191.png" alt="image-20241029164446191"></p>
<h5 id="其他的激活函数："><a href="#其他的激活函数：" class="headerlink" title="其他的激活函数："></a>其他的激活函数：</h5><h6 id="ReLU："><a href="#ReLU：" class="headerlink" title="ReLU："></a>ReLU：</h6><p><img src="/2024/11/03/Week3-ML/image-20241029170727481.png" alt="image-20241029170727481"></p>
<h6 id="线性激活函数：（相当于没有激活函数g（z）-z）"><a href="#线性激活函数：（相当于没有激活函数g（z）-z）" class="headerlink" title="线性激活函数：（相当于没有激活函数g（z）&#x3D;z）"></a>线性激活函数：（相当于没有激活函数g（z）&#x3D;z）</h6><p><img src="/2024/11/03/Week3-ML/image-20241103151318238.png" alt="image-20241029170926167"></p>
<h5 id="如何选择激活函数？"><a href="#如何选择激活函数？" class="headerlink" title="如何选择激活函数？"></a>如何选择激活函数？</h5><p>输出层：</p>
<p>当输出为零或者一的分类问题，选择sigmoid</p>
<p>如果为回归问题，选择线性激活函数</p>
<p>如果回归问题中的输出永远为非负数，则用ReLU</p>
<p>隐藏层：</p>
<p>最常用的是ReLU</p>
<p>why？</p>
<p>（1）ReLU计算速度更快</p>
<p>（2）ReLU只在一个地方变平（左边），sigmoid在两边都会变平，越平坦的地方梯度越小，梯度下降的时候更新越慢。</p>
<p>激活函数的选择和损失函数的计算好像有关系，损失函数包含了激活函数的形式。</p>
<p>反向传播迭代时，有一项是激活函数的导数，导数小，迭代会很慢</p>
<h5 id="为什么需要激活函数？"><a href="#为什么需要激活函数？" class="headerlink" title="为什么需要激活函数？"></a>为什么需要激活函数？</h5><p>如果只用线性激活函数：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029173128258.png" alt="image-20241029173128258"></p>
<p>无论有多少层，效果和一层没什么区别，神经网络就完全等同于线性回归</p>
<p>理解为什么只有线性不行，但不理解为什么有了非线性就行了</p>
<p>假设我们现在要使用神经网络模拟如图所示的一个函数：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029173946366.png" alt="image-20241029173946366"></p>
<p>假设我们现在要使用神经网络模拟如图所示的一个函数，设计了如下神经网络：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029174246730.png" alt="image-20241029174246730"></p>
<p>如果全是线性：</p>
<p><strong>从输入层到隐层，实际是三条直线</strong>，</p>
<p>z1&#x3D;w1x+b1</p>
<p>z2&#x3D;w2x+b2</p>
<p>z3&#x3D;w3x+b3</p>
<p>而隐层到输出层呢？<strong>其实就是把上面的三条线加起来</strong>，即：</p>
<p>y&#x3D;w4z1+w5z2+w6z3+b4</p>
<p>绘制3条直线，看看加起来能不能模拟上述的分段函数：</p>
<p>其实还是一条直线</p>
<p>y&#x3D;w4z1+w5z2+w6z3+b4&#x3D;(w1w4+w2w5+w3w6)x+(b1w4+b2w5+b3w6+b4)&#x3D;ax+b</p>
<p>为了要让神经网络能模拟复杂的函数（非线性的），所以要让神经元引入非线性的激活函数。</p>
<p>假设要拟合上面的分段函数，可以使用如下函数对其进行相加：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029174456445.png" alt="image-20241029174456445"></p>
<p>如果神经网络能模拟出黄色线和橙色线，那么让其相加，就可以得出 f ff 函数。而这两可以和sigmoid很像</p>
<p><strong>任何复杂的函数都可以由一个常量加一堆sigmoid函数模拟出来</strong></p>
<p><img src="/2024/11/03/Week3-ML/image-20241029174605988.png" alt="image-20241029174605988"></p>
<p><img src="/2024/11/03/Week3-ML/image-20241029174756023.png" alt="image-20241029174756023"></p>
<p><strong>通过神经网络，模拟出了两条非线性的sigmoid函数，然后将其合并（相加），最终模拟出函数 f 。</strong></p>
<h5 id="多分类问题softmax："><a href="#多分类问题softmax：" class="headerlink" title="多分类问题softmax："></a>多分类问题softmax：</h5><p>softmax实际上是sigmiod与逻辑回归的泛化</p>
<p><img src="/2024/11/03/Week3-ML/image-20241029204810754.png" alt="image-20241029204810754"></p>
<h6 id="损失函数的计算："><a href="#损失函数的计算：" class="headerlink" title="损失函数的计算："></a>损失函数的计算：</h6><p><img src="/2024/11/03/Week3-ML/image-20241029204941856.png" alt="image-20241029204941856"></p>
<p>SparseCategoricalCrossentropy</p>
<h6 id="Softmax的改进："><a href="#Softmax的改进：" class="headerlink" title="Softmax的改进："></a>Softmax的改进：</h6><p>由于会出现浮点数计算精度变化问题，所以可以采用合并的方法减少计算带来的误差：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030143836542.png" alt="image-20241030143836542"></p>
<p>更好的调用方法：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030144603227.png" alt="image-20241030144603227"></p>
<h6 id="多标签与多分类的区别"><a href="#多标签与多分类的区别" class="headerlink" title="多标签与多分类的区别"></a>多标签与多分类的区别</h6><p>多个标签分类（mutilabel）：输出有多个种类</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030145400578.png" alt="image-20241030145400578"></p>
<p>多分类（muticlass）：输出有多种情况，0，1，2……</p>
<h5 id="优化方法："><a href="#优化方法：" class="headerlink" title="优化方法："></a>优化方法：</h5><p>Adam：可以自适应变化学习率，而不是固定学习率，当学习率过小时，增加学习率，当学习率过大时，减小学习率</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030152405672.png" alt="image-20241030152405672"></p>
<p>更好的调用方法：使用Adam</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030152446843.png" alt="image-20241030152446843"></p>
<h5 id="网络层类型："><a href="#网络层类型：" class="headerlink" title="网络层类型："></a>网络层类型：</h5><p>convolutional layer（卷积层）</p>
<p>只选取前一层的部分（小窗口）作为输入而不是全部</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030153802778.png" alt="image-20241030153802778"></p>
<p>反向传播（建立在梯度下降法基础上）：如图，从J从右到左，先计算j关于d的导数，然后计算j关于a的导师，链式传导</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030155237260.png" alt="image-20241030155237260"></p>
<h5 id="反向传播："><a href="#反向传播：" class="headerlink" title="反向传播："></a>反向传播：</h5><p>为什么要用反向传播呢？</p>
<p>如果有n个节点p个参数，只需要计算大概n+p步就行，而不是n*p步：</p>
<p>怎么理解呢？反向传播的时候是倒着遍历一次就能计算出所有导数，复杂度n+p，如果每个导数都单独计算的话，复杂度n*p</p>
<p>如果我不知道反向传播算法，我应该会用如下式子的近似求导方法：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030160643815.png" alt="image-20241030160643815"></p>
<p>​		即要求某个参数的导数就让这个参数微变一点点，然后求出结果相对于参数变化量的比值。那么为何我们的神经网络算法没有采用这种方法求导呢？<br>​		让我们再看一看下图的例子，现在假设输入向量经过正向传播后，现在要求出参数w1和w2的导数，按照上述方法计算时，对w1微扰后，需要重新计算红框内的节点；对w2微扰后，需要重新计算绿框内的节点。这两次计算中也有大量的“重复单元”即图中的蓝框，实际上神经网络的每一个参数的计算都包含着这样大量的重复单元，那么神经网络规模一旦变大，这种算法的计算量一定爆炸，没有适用价值。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241030160842551.png" alt="image-20241030160842551"></p>
<p>​		我们如果选了一条相反的计算线路解决重复计算的问题。在神经网络中，那些牛逼的大神同样选择了另一条计算方向去计算梯度，这个方向就是从后向前“反向”地计算各层参数的梯度。这个计算方向能够高效的根本原因是，在计算梯度时前面的单元是依赖后面的单元的计算，而“从后向前”的计算顺序正好“解耦”了这种依赖关系，先算后面的单元，并且记住后面单元的梯度值，计算前面单元之就能充分利用已经计算出来的结果，避免了重复计算。</p>
<h4 id="模型评估："><a href="#模型评估：" class="headerlink" title="模型评估："></a>模型评估：</h4><p>将数据集分为测试集与训练集：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031145035338.png" alt="image-20241031145035338"></p>
<p>如果我们的训练集的损失函数十分的小，那么测试集的损失函数就可能非常的大，因为会出现过拟合。</p>
<h5 id="模型选择（交叉验证测试集）："><a href="#模型选择（交叉验证测试集）：" class="headerlink" title="模型选择（交叉验证测试集）："></a>模型选择（交叉验证测试集）：</h5><p><img src="/2024/11/03/Week3-ML/image-20241103151453347.png" alt="image-20241031150334692"></p>
<p>基于test set，我们通过选择最小的j<del>test</del>来确定最合适d的值，那么这个d也非常契合这个test set，那么在test set上对应的误差值要比一般情况要小。所以这种方式选出来的是有缺陷的。</p>
<h6 id="交叉验证测试集："><a href="#交叉验证测试集：" class="headerlink" title="交叉验证测试集："></a>交叉验证测试集：</h6><p>训练集：</p>
<p>交叉验证集：</p>
<p>测试集：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031151650090.png" alt="image-20241031151650090"></p>
<p>train用来寻找<strong>最合适w和b</strong>，而cv用来找<strong>几阶多项式最合适</strong>，而test用来评价模型在数据集中的<strong>泛化性</strong>。（通过test和cv来做决定，而决定后用测试集评估）</p>
<h5 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h5><p><img src="/2024/11/03/Week3-ML/image-20241031153205803.png" alt="image-20241031153205803"></p>
<p>如果J<del>train</del>和J<del>cv</del>都高，说明模型<strong>欠拟合</strong>，具有<strong>高偏差（拟合能力差）</strong>，如果J<del>train</del>低而J<del>cv</del>高（或者说两者差异较大时），说明模型<strong>过拟合（泛化能力差）</strong>，具有高方差，只有J<del>train</del>和J<del>cv</del>都很低时，才说明具体泛化性。在线性回归中，同时具有高偏差和高方差不太常见，但在神经网络中可能存在高偏差和高方差的情况，J<del>train</del>很高，而J<del>cv</del>更高</p>
<h6 id="与多项式系数的关系："><a href="#与多项式系数的关系：" class="headerlink" title="与多项式系数的关系："></a><strong>与多项式系数的关系：</strong></h6><p><img src="/2024/11/03/Week3-ML/image-20241031153856879.png" alt="image-20241031153856879"></p>
<h6 id="与λ正则化参数的关系："><a href="#与λ正则化参数的关系：" class="headerlink" title="与λ正则化参数的关系："></a><strong>与λ正则化参数的关系：</strong></h6><p>而对于λ正则化参数的选择，λ过低会导致过拟合，过高会导致高偏差，</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031155807202.png" alt="image-20241031155807202"></p>
<h6 id="如何制定一个性能评价基准（Jtrain和Jcv多高算高，多低才算低）："><a href="#如何制定一个性能评价基准（Jtrain和Jcv多高算高，多低才算低）：" class="headerlink" title="如何制定一个性能评价基准（Jtrain和Jcv多高算高，多低才算低）："></a><strong>如何制定一个性能评价基准（J<del>train</del>和J<del>cv</del>多高算高，多低才算低）：</strong></h6><p>（1）人类的表现水平</p>
<p>（2）对手算法的表现</p>
<p>（3）靠预期猜想     </p>
<p><img src="/2024/11/03/Week3-ML/image-20241031161451874.png" alt="image-20241031161451874"></p>
<p>主要通过差值大小比较基准表现和训练误差相差小于训练误差和验证误差时，高方差。反之则高偏差。</p>
<h6 id="学习曲线："><a href="#学习曲线：" class="headerlink" title="学习曲线："></a><strong>学习曲线：</strong></h6><p>J<del>train</del>会随train数不断增加，点越多，你的线就越难完全穿过所有点。而J<del>cv</del>会不断减小，拟合程度不断在增大。但是J<del>cv</del>是会永远大于J<del>train</del>的，比较参数是根据J<del>train</del>来的。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031162008131.png" alt="image-20241031162008131"></p>
<p><strong>高偏差时：</strong></p>
<p>更多的训练集不会有太多帮助，数据只会接近真实的拟合函数因为你函数都没拟合好，数据再多也是高偏差</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031162528445.png" alt="image-20241031162528445"></p>
<p><strong>高方差时：</strong></p>
<p>更多的训练集会有帮助：过拟合是因为模型能力强，数据少，所以加数据量会有所帮助。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031162923552.png" alt="image-20241031162923552"></p>
<h6 id="当出现高方差时解决方法："><a href="#当出现高方差时解决方法：" class="headerlink" title="当出现高方差时解决方法："></a><strong>当出现高方差时解决方法：</strong></h6><p>（1）获取更多的训练集</p>
<p>（2）尝试更少的特征</p>
<p>（3）减小正则化参数</p>
<h6 id="当出现高偏差时解决方法："><a href="#当出现高偏差时解决方法：" class="headerlink" title="当出现高偏差时解决方法："></a><strong>当出现高偏差时解决方法：</strong></h6><p>（1）尝试更多的特征</p>
<p>（2）增加维度（加x^2^之类的）</p>
<p>（3）增加正则化参数</p>
<h6 id="在神经网络中-出现高方差和高偏差时的解决方法："><a href="#在神经网络中-出现高方差和高偏差时的解决方法：" class="headerlink" title="在神经网络中,出现高方差和高偏差时的解决方法："></a>在神经网络中,出现高方差和高偏差时的解决方法：</h6><p><img src="/2024/11/03/Week3-ML/image-20241031171927258.png" alt="image-20241031171927258"></p>
<p>可以通过构建更大的神经网络来减小J<del>train</del>，也可以通过获取更多的数据来减少J<del>cv</del>。</p>
<p>构建更大的神经网络不会导致高方差吗？</p>
<p>只要正则化合理，就不会出现过度拟合，大神经网络比小神经网络甚至可能得到的方差更小。但是会更慢且更贵</p>
<p>如何正则化：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031172739093.png" alt="image-20241031172739093"></p>
<p>L1，L2，L3时正则化方式，L1是参数绝对值，L2是参数平方，L3是两者相加。</p>
<h5 id="误差分析："><a href="#误差分析：" class="headerlink" title="误差分析："></a>误差分析：</h5><p>通过手动检查算法中的一组示例，找出错误分类或错误标签的，进行分类，找出误差的原因，从而找到改进方向</p>
<p>添加更大数据：</p>
<h6 id="数据增强："><a href="#数据增强：" class="headerlink" title="数据增强："></a>数据增强：</h6><p>利用现有的训练示例修改成一个新的训练示例。 </p>
<p>数据增强要求你所改变的数据是能经常在测试集中出现的样子，比如语言识别你可以加入在车上的噪音，但如果你加入在火箭上的噪音，就没有多少意义了。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031200411712.png" alt="image-20241031200411712"></p>
<h6 id="数据合成："><a href="#数据合成：" class="headerlink" title="数据合成："></a>数据合成：</h6><p>创建全新的示例（常用于计算机视觉）</p>
<p>比如识别文字，可以通过计算机中不同的字体，不同颜色，不同对比度和非常不同的字生成和现实中一样的数据示例</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031201002864.png" alt="image-20241031201002864"></p>
<p>AI&#x3D;Code+Data</p>
<h6 id="迁移学习（常用于CV和NLP）："><a href="#迁移学习（常用于CV和NLP）：" class="headerlink" title="迁移学习（常用于CV和NLP）："></a>迁移学习（常用于CV和NLP）：</h6><p>输出层不一样没关系，也可以套用其他图像分类的隐藏层：比如你可以通过一个大的识别猫狗人的神经网络迁移到一个识别手写数字的神经网络</p>
<p>为什么？</p>
<p>两个网络的本质都是图像分类，所以在隐藏层的大部分工作都是相似的，相当于螺丝刀的刀柄都是一样的，但把刀头进行替换作用就不一样了</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031203103782.png" alt="image-20241031203103782"></p>
<p>（1）只训练输出层参数（small data）：</p>
<p>隐藏层参数不变，使用随机梯度下降或Adam仅更新输出层参数</p>
<p>（2）训练全部参数（big data）：<br>隐藏层参数以参照为初始化</p>
<p>监督预训练：在一个非常大的数据集上训练</p>
<p>微调：在其中获取想要的参数，进一步进行梯度下降微调权重以适应自己这个小的模型</p>
<h5 id="机器学习项目周期："><a href="#机器学习项目周期：" class="headerlink" title="机器学习项目周期："></a>机器学习项目周期：</h5><p>需求分析-&gt;数据收集-&gt;训练模型（可能会改进收集的数据）-&gt;部署模型（如果效果不好可能会通过用户数据重新改进模型或数据）</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031205505439.png" alt="image-20241031205505439"></p>
<h5 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h5><h6 id="precision（准确率）和recall（召回率）与F1-score："><a href="#precision（准确率）和recall（召回率）与F1-score：" class="headerlink" title="precision（准确率）和recall（召回率）与F1 score："></a>precision（准确率）和recall（召回率）与F1 score：</h6><p>用于倾斜数据集或者稀有数据集：即输出1和0的概率远远不等时</p>
<p>混淆矩阵</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031211243150.png" alt="image-20241031211243150"></p>
<p>高精度说明预测输出1的结果是准确的。高召唤率说明能正确识别出输出1的数据。</p>
<p>提高阈值，会有较高精度，较小召回率，</p>
<p>降低阈值，则会有较低精度，较大召回率</p>
<p><img src="/2024/11/03/Week3-ML/image-20241031212351079.png" alt="image-20241031212351079"></p>
<p>Average&#x3D;P（精度）+R（召回率）</p>
<p>F1 score&#x3D;2PR&#x2F;R+P(P和R的调和)</p>
<h4 id="决策树（分类问题）："><a href="#决策树（分类问题）：" class="headerlink" title="决策树（分类问题）："></a>决策树（分类问题）：</h4><p><img src="/2024/11/03/Week3-ML/image-20241101131542357.png" alt="image-20241101131542357"></p>
<p>分为根节点，决策节点，叶子节点。（实际就是一个if-else结构）</p>
<h5 id="Q1：该选择什么特征来进行区分每个节点呢？"><a href="#Q1：该选择什么特征来进行区分每个节点呢？" class="headerlink" title="Q1：该选择什么特征来进行区分每个节点呢？"></a>Q1：该选择什么特征来进行区分每个节点呢？</h5><p>应满足最大纯度（一个特征能最大程度得到你所想得到的分类，比如通过NDA分类猫狗）</p>
<h5 id="Q2：什么时候停止分叉？"><a href="#Q2：什么时候停止分叉？" class="headerlink" title="Q2：什么时候停止分叉？"></a>Q2：什么时候停止分叉？</h5><p>当一个节点纯度达到100%</p>
<p>当分裂没有进一步的结果导致达到树的最大深度</p>
<p>当分类对纯度的改善低于某个阈值</p>
<p>当一个节点中示例的数量低于某个阈值，导致其不能分成更小的子集</p>
<h5 id="量化纯度："><a href="#量化纯度：" class="headerlink" title="量化纯度："></a>量化纯度：</h5><p>信息熵（评价一个样本中的混乱程度）：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101133618106.png" alt="image-20241101133618106"></p>
<p><strong>疑问：熵的求解方式和逻辑回归中的损失函数公式类似，两者是否有啥联系？</strong></p>
<p><img src="/2024/11/03/Week3-ML/image-20241023205352950.png" alt="image-20241023205352950"></p>
<p>逻辑回归函数可以看作是将特征和权重映射成f<del>w，b</del>是一个概率，而y则是等于1或0，可以看出如果f<del>w，b</del>越接近0或1，就越接近上图的左边或者右边，也就是说明损失函数越小</p>
<h5 id="信息增益：熵的减少"><a href="#信息增益：熵的减少" class="headerlink" title="信息增益：熵的减少"></a>信息增益：熵的减少</h5><p>以下图为例子：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101135901979.png" alt="image-20241101135901979"></p>
<p>![image-20241101140305717](C:\Users\the xie\Desktop\ML\DL\image-20241101140305717.png)</p>
<p>信息增益根节点熵值减去两个分支的熵求加权平均，最后取熵减最快的情况进行分支</p>
<h5 id="独热编码："><a href="#独热编码：" class="headerlink" title="独热编码："></a>独热编码：</h5><p>将非数字特征通过编码转换成数字的形式</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101142607646.png" alt="image-20241101142607646"></p>
<h5 id="对于连续的数值，我们又该怎么划分子树呢？"><a href="#对于连续的数值，我们又该怎么划分子树呢？" class="headerlink" title="对于连续的数值，我们又该怎么划分子树呢？"></a>对于连续的数值，我们又该怎么划分子树呢？</h5><p>取不同的阈值，寻找一个使熵减最大的阈值，进行划分。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101150623354.png" alt="image-20241101150623354"></p>
<h4 id="回归树（回归问题）："><a href="#回归树（回归问题）：" class="headerlink" title="回归树（回归问题）："></a>回归树（回归问题）：</h4><p>类似于决策树，计算根节点的方差减去两边方差的加权平均，得出方差减少得最快的情况</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101151251196.png" alt="image-20241101151251196"></p>
<p>最终结果通过平均值表示</p>
<p><img src="/2024/11/03/Week3-ML/image-20241103151729418.png" alt="image-20241101151436408"></p>
<h4 id="复合树模型："><a href="#复合树模型：" class="headerlink" title="复合树模型："></a>复合树模型：</h4><p>为什么需要复合树？</p>
<p>当你改变一个数据示例的话，可能会因为熵减变了，导致分叉情况变化。</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101155745249.png" alt="image-20241101155745249"></p>
<p>而复合树就是把所有合理的决策树放一起，然后每个进行预测，最后进行投票选出答案</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101155917298.png" alt="image-20241101155917298"></p>
<h5 id="随机森林："><a href="#随机森林：" class="headerlink" title="随机森林："></a>随机森林：</h5><h6 id="Bagging算法"><a href="#Bagging算法" class="headerlink" title="Bagging算法"></a>Bagging算法</h6><p>Bagging算法3是一种集成学习算法，其全称为自助聚集算法（<strong>B</strong>ootstrap <strong>agg</strong>regat<strong>ing</strong>）</p>
<p><img src="/2024/11/03/Week3-ML/1086acbf613ccf40deec32d5f6e616de.png" alt="img"></p>
<p>算法的具体步骤为：假设有一个大小为 N 的训练数据集，每次从该数据集中有放回的取选出大小为 M 的子数据集，一共选 K 次，根据这 K 个子数据集，训练学习出 K 个模型。当要预测的时候，使用这 K 个模型进行预测，再通过取平均值或者多数分类的方式，得到最后的预测结果。</p>
<h6 id="随机森林算法："><a href="#随机森林算法：" class="headerlink" title="随机森林算法："></a>随机森林算法：</h6><p>将多个决策树结合在一起，每次数据集是随机有放回的选出，同时随机选出部分特征作为输入，所以该算法被称为随机森林算法。可以看到随机森林算法是以决策树为估计器的Bagging算法。</p>
<p><img src="/2024/11/03/Week3-ML/b94d67cbf2f588dd06efcc15271ee369.png" alt="2.png"></p>
<p>假设训练集 T 的大小为 N ，特征数目为 M ，随机森林的大小为 K ，随机森林算法的具体步骤如下：</p>
<p>遍历随机森林的大小 K 次：<br>  从训练集 T 中有放回抽样的方式，取样N 次形成一个新子训练集 D<br>  随机选择 m 个特征，其中 m &lt; M<br>  使用新的训练集 D 和 m 个特征，学习出一个完整的决策树<br>得到随机森林</p>
<p>  上面算法中 m 的选择：对于分类问题，可以在每次划分时使用根号M 个特征，对于回归问题， 选择M&#x2F;3 但不少于 5 个特征。<br>提升树（boosting tre）：</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101163042723.png" alt="image-20241101163042723"></p>
<p>其核心是刻意增加被错误分类的示例的概率，以多做错题来进行刻意学习，其基本实现方式就是提高那些被前一轮分类器错误分类样本的权值，而降低那些被正确分类样本的权值。</p>
<p>常用的提升树算法有：GBT，GBRT，XGboost</p>
<p><img src="/2024/11/03/Week3-ML/image-20241101165516242.png" alt="image-20241101165516242"></p>
<h6 id="决策树与神经网络的区别："><a href="#决策树与神经网络的区别：" class="headerlink" title="决策树与神经网络的区别："></a>决策树与神经网络的区别：</h6><p>决策树适用于结构化数据，像以DataFrame形式存储的数据，而神经网络适用所有数据包括非结构化数据，像什么图像、语言、文本。决策树速度非常快，而神经网络需要大量时间训练。小型决策树是可解释的 </p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Zhen Xie</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2024/11/03/Week3-ML/">http://example.com/2024/11/03/Week3-ML/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2024/10/27/Week2-ML/">Week2 ML</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Zhen Xie | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>